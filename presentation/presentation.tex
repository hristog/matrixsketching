\documentclass[first=dgreen,second=purple,logo=redque]{aaltoslides}
%\documentclass{aaltoslides} % DEFAULT
%\documentclass[first=purple,second=lgreen,logo=redque,normaltitle,nofoot]{aaltoslides} % SOME OPTION EXAMPLES

\usepackage[latin9]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{amssymb,amsmath}
\usepackage{url}
\usepackage{lastpage}
\usepackage{subfigure}
\usepackage{algpseudocode}
\usepackage{xcolor}

\definecolor{dgreen}{rgb}{0.,0.6,0.}

\newcommand\overmat[2]{%
  \makebox[0pt][l]{$\smash{\color{white}\overbrace{\phantom{%
    \begin{matrix}#2\end{matrix}}}^{\text{\color{black}#1}}}$}#2}
\newcommand\bovermat[2]{%
  \makebox[0pt][l]{$\smash{\overbrace{\phantom{%
    \begin{matrix}#2\end{matrix}}}^{\text{#1}}}$}#2}
\newcommand\partialphantom{\vphantom{\frac{\partial e_{P,M}}{\partial w_{1,1}}}}

\title{Simple and Deterministic Matrix Sketching\footnote{Authored by Edo Liberty and and won the \textit{KDD-2013 best paper} award\cite{Liberty13}.}}

\author[H. Georgiev and H. Shen]{Hristo Georgiev and Huibin Shen}
\institute[ICS]{Department of Information and Computer Science\\
Aalto University, School of Science}

\aaltofootertext{H. Georgiev and H. Shen}{T-61.6020}{\arabic{page}/\pageref{LastPage}\ }

%\date{}
\newcommand{\vectornorm}[1]{\left\|#1\right\|}
\newcommand{\IndState}{\State\hspace{\algorithmicindent}}

\everymath\expandafter{\the\everymath \color{blue}}

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\aaltotitleframe

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Background section
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Content}
\begin{itemize}
\item Background
\item \textcolor{gray}{Related work}
\item \textcolor{gray}{Frequent directions}
\item \textcolor{gray}{Experiments and Results}
\item \textcolor{gray}{Conclusion}
\end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%\begin{frame}{Background}
%\begin{itemize}
%  \item Many applications are based on matrix, \emph{e.g.} search engine (document-term matrix), social networks (adjacency matrix).
%  \item In the ``Big data'' setting, matrix could be very large.
%  \item One way to handle massive data is to utilize map-reduce like programming model.
%\end{itemize}
%\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\begin{frame}{Background}
%\begin{itemize}
%  \item This paper: approximate the original matrix by a much smaller one while still preserving the correlation (if assume centering in the feature space). 
%  \item Formally, consider a large matrix $A \in \mathbb{R}^{n\times m}$ with $n$ rows and $m$ columns, a sketch matrix $B \in \mathbb{R}^{\ell \times m}$ containing only $\ell \ll n$ rows such that $A^TA \approx B^TB$. 

%This paper shows (Proof shown later):
%\begin{align}
%B^TB \prec A^TA \quad \text{and} \quad || A^TA-B^TB || \leq 2||A||_f^2/\ell \nonumber
%\end{align}
%\end{itemize}
%\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{What is a sketch?}
\begin{itemize}
  \item A \textcolor{red}{sketch} of a matrix $A$ is another matrix $B$ which is significantly
  \textcolor{dgreen}{smaller} than $A$, but still approximates it \textcolor{dgreen}{well}.
  \item A \textcolor{red}{good} sketch matrix is one on which some computations can
  be performed, \textcolor{dgreen}{without} \textit{much} loss of precision.
   %\emph{(as opposed to performing them on the original matrix)}
\end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[allowframebreaks=1]{Why would one need a sketch?}
\begin{itemize}
  \item Modern data sets are often viewed as \textcolor{dgreen}{matrices}
  %\emph{(They are often extremely large)}
  \item{Examples:}
  \begin{itemize}
    \item Textual data in the bag-of-words model %\emph{(Where the rows correspond to documents)}
    \item Large-scale image analysis %\emph{(Each row corresponds to one image, and contains either pixel values, or other derived feature values)}
  \end{itemize}
  \item Low rank approximations are used in PCA, LSI, \textit{k}-means clustering, etc.
%\end{itemize}

%\framereak

%\begin{itemize}
  \item Typically, compute the \textcolor{red}{SVD} of some large matrix $A$
  \item Then \textcolor{dgreen}{approximate} using the first $k$ singular vectors
  \begin{itemize}
     \item where $k \geq t$, i.e. only interested in unit vectors such
     that $\vectornorm{Ax} \geq t$.
  \end{itemize}
  \item However, the \textcolor{red}{distributed} nature of such matrices renders SVD \textcolor{dgreen}{infeasible}
\end{itemize}

\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Properties of matrix sketching methods}
Designed to be \textcolor{red}{pass-efficient}
\begin{itemize}
\item i.e. data can be read only a \textcolor{dgreen}{constant} number of times.
\end{itemize}
The \textcolor{red}{streaming model}: \textcolor{dgreen}{only one} pass is permitted!
\begin{itemize}
\item Sketching becomes more challenging, since each row can be processed only
once. %\emph{(further, storage is severely limited)}
\end{itemize}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Related work section
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Content}
\begin{itemize}
\item \textcolor{gray}{Background}
\item Related work
\item \textcolor{gray}{Frequent directions}
\item \textcolor{gray}{Experiments and Results}
\item \textcolor{gray}{Conclusion}
\end{itemize}
\end{frame}

\begin{frame}[allowframebreaks=1]{Existing approaches}
There are three main approaches:
\begin{itemize}
\item \textcolor{red}{random-projection:} encaptures two classes of methods
\begin{enumerate}[(i)]
  \item Generate a \textcolor{blue}{sparser} version of the matrix.
   %\emph{\begin{itemize}
   %\item It can then be stored \textit{more efficiently}, and
   %\item can be multiplied \textit{faster} by other matrices.
   %\end{itemize}}
  \item \textcolor{dgreen}{Randomly combine} matrix rows.
\end{enumerate}
\item \textcolor{red}{hashing:} 
	\begin{itemize}
		\item Simple and efficient \textcolor{blue}{subspace embeddings} applied in $O(nnz(A))$ time, for any matrix $A$.
	\end{itemize}
\item \textcolor{red}{sampling:} \textcolor{dgreen}{Column Subset Selection Problem}:
%\emph{in the context of the \textit{Column Subset Selection Problem}:}
\begin{itemize}
  \item find a \textcolor{blue}{small subset} of matrix rows (or columns) that approximates
  the entire matrix
  %\emph{\item solved using a simple streaming solution:}
   \begin{itemize}
   \item \textcolor{dgreen}{sample} rows from the input matrix with probability \textcolor{blue}{proportional} to
   their squared $\ell_{2}$ norm
   \end{itemize}
\end{itemize}
\end{itemize}

Proposed fourth approach, \textcolor{red}{Frequent-directions}.

\end{frame}

\begin{frame}[allowframebreaks=1]{Item frequency estimation}
\begin{itemize}
    \item Used to uncover \textcolor{dgreen}{frequent} items in an item stream
    \item (Re-)Invented (at least!) four times \cite{Misra82, Demaine02, Karp03, Metwally05}
	\item Referred to as \textcolor{dgreen}{\textit{Frequent-items}}
\end{itemize}
\begin{itemize}
	\item \textbf{Goal:} use $O(\ell)$ space to produce estimates $g_{j}$, such
	that
	\begin{itemize}
		\item{$|f_{j} - g_{j} | \leq n/l$}, for all $j$ \textcolor{red}{simultaneously}
	\end{itemize}
\end{itemize}

\framebreak

\begin{itemize}
	\item The algorithm:
	\begin{algorithmic}
		\State \textbf{Input}:\begin{itemize}\item $d$ \textcolor{dgreen}{items} $a_{1}$, $a_{2}$, \ldots,
		$a_{d}$
		\item $n$ \textcolor{dgreen}{item appearances} $A_{1}$, $A_{2}$, \ldots,
		$A_{n}$\end{itemize}
		\\\State \textbf{Repeat} until there are \textcolor{red}{less than}
		$\ell$ \textcolor{dgreen}{unique} items left \{
		\begin{itemize}
			\item \textcolor{red}{Get} item $A_{j}$ from stream, for $j = 1 \ldots n$
		\end{itemize}
		\begin{itemize}
			\item \textcolor{blue}{\textit{If}} there are free slots among $\ell$
			\begin{itemize}\item \textcolor{red}{Create} new bucket for item type $k$ and \textcolor{red}{store} the item there\end{itemize}
			\item \textcolor{blue}{\textit{Else}}
				\begin{itemize}
					\item \textcolor{red}{Find} \textcolor{dgreen}{median} count $\delta = f_{\ell/2}$
					of items, and
					\item \textcolor{red}{Remove} exactly $\min{(\delta, f_{i})}$ appearances from each bucket $i = 1 \ldots \ell$
				\end{itemize}
		\end{itemize}
		\State \}
	\end{algorithmic}
\end{itemize}
\end{frame}

\begin{frame}[allowframebreaks=1]{Item frequency estimation}


\textbf{Claim:}

\begin{itemize}
\item \textbf{If} item $a_{j}$ appears in the \textcolor{dgreen}{final} trimmed
stream $g_{j}$ times
\begin{itemize}
  \item \textbf{Then} $g_{j}$ is a \textcolor{red}{good approximation} for its
  true frequency $f_{j}$.%\emph{(even in the case of $g_{j}$ = 0).}
\end{itemize}         
\end{itemize}

\begin{proof}
\begin{itemize}
   \item Each item-type is deleted \textcolor{dgreen}{at most} once per iteration:
   \begin{itemize} \item $f_{j} - g_{j} \leq t$, where $t$ is the
   number of deleted items.\end{itemize}
%\end{itemize}

%\framebreak

%\textbf{Proof (cont.):}

%\begin{itemize}
   %\item \textcolor{gray}{Each item-type is deleted at most once per iteration:}
   %\begin{itemize} \item \textcolor{gray}{$f_{j} - g_{j} \leq t$, where $t$ is the number of deleted items.}\end{itemize}
   \item $\ell$ items are deleted in every batch, and the total number of
   deleted items is \textcolor{red}{upper-bounded} by $n$, therefore:
   \begin{itemize} \item $t\ell \leq n$, or $t \leq n/\ell$\cite{Misra82}.\end{itemize} 
\end{itemize}
\end{proof}
\begin{itemize}
  \item If one sets $\ell > 1/\varepsilon$,
  \begin{itemize} \item Then any item that appears \textcolor{dgreen}{more than}
  $\varepsilon n$ times in the stream \textcolor{red}{must} appear in
  the final sketch.\end{itemize}
\end{itemize}
  
\end{frame}

\begin{frame}[allowframebreaks=1]{Connection to \textit{sampling}}

\begin{itemize}
   \item \textcolor{red}{All} frequencies can be approximated from a
   \textcolor{dgreen}{uniform} sample of the stream
   \begin{itemize}
		\item using \textcolor{red}{\textit{Chernoff's bound}}, and then applying the \textcolor{red}{\textit{union
   bound}}
		\item $O(r\log{(r)}/\varepsilon^{2})$ samples suffice to ensure that
   
		\begin{itemize}
			\item $|f_{i} - g_{i}| \leq \varepsilon f_{\text{max}}$, where $r = n / f_{\text{max}}$
		\end{itemize}
		\item same complexity in \textcolor{red}{sketching}:
		\begin{itemize}
			\item but $r = \vectornorm{A}_{f}^{2} /
				\vectornorm{A}_{2}^{2}$,
			\item guaranteeing that $\vectornorm{A^{T}A - B^{T}B} \leq \varepsilon
				\vectornorm{A^{T}A}$.
		\end{itemize}
	\end{itemize}		
\end{itemize}

\end{frame}

%\begin{frame}[allowframebreaks=1]{\textit{Frequent-items} in matrix sketching}

%\begin{itemize}
  %\item \textbf{Input:} matrix $A$, consisting of a stream of
  %\textcolor{dgreen}{rows} $A_{i}$,
%  	\begin{itemize}\item where $A_{i} \in \{ e_{1}, \ldots, e_{m} \}$, for
  	%$e_{j}$ being the $j$th \textcolor{dgreen}{standard basis} vector
%  	
  	%\item $A_{j} = e_{j}$, if the $i$th row of the matrix
  	%\item Frequency: $f_{j} = \vectornorm{Ae_{j}}^{2}$\\
  %\end{itemize}
  %\item \textbf{Output:} $g_{j} = \vectornorm{Be_{j}}^{2}$, which is a
%  	\textcolor{red}{good approximation} to $f_{j}$
  %\begin{itemize}\item Let $n = \vectornorm{A}_{f}^{2}$, then $|f_{j} -
  %g_{j}| \leq n / \ell$, which is equivalent to
  %\begin{itemize}\item $\vectornorm{Ae_{j}}^{2} -
  %\vectornorm{Be_{j}}^{2} \leq \vectornorm{A}_{j}^{2} / \ell$\end{itemize}
  %\end{itemize}
%\end{itemize}

%\end{frame}

%\begin{frame}[allowframebreaks=1]{Connection to \textit{low-rank approximation}}
%
%\begin{itemize}
   %\item The goal is to obtain a \textcolor{red}{small} matrix $B$,
   %\begin{itemize}
     %\item consisting of $\ell$ rows, that contains in its row space
   	 %\item a projection matrix $\Pi_{B,\xi}^{A}$ of rank $k$ such that 
   	 %\begin{itemize}
   	   %\item $\vectornorm{A - A\Pi_{B,\xi}^{A}}_{\xi} \leq (1+\varepsilon)
   	   %\vectornorm{A - A_{k}}_{\xi}$, 

   	     	%\item where $A_{k}$ is the \textcolor{red}{best}
   	   %rank-$k$ approximation of $A$, and
   	   		%\item $\xi$ is either $2$ or $f$%\emph{(i.e. either the spectral norm or the Frobenius norm)}.
   	 %\end{itemize}
   %\end{itemize}
   %\item \textbf{Note:} \textit{Frequent-directions} can be used to
   %\textcolor{red}{produce} a low-rank approximation result.
   %\begin{itemize} \item \textit{Lemma 4} from a modified version
   %of [12].\end{itemize}
%\end{itemize}
%\framebreak
%\textbf{Lemma: }
%\begin{itemize}
    %\item \ldots
%\end{itemize}
%
%\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Frequent directions section
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Content}
\begin{itemize}
\item \textcolor{gray}{Background}
\item \textcolor{gray}{Related work}
\item Frequent directions
\item \textcolor{gray}{Experiments and Results}
\item \textcolor{gray}{Conclusion}
\end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[allowframebreaks=1]{The \textit{Frequent-directions} algorithm}

%\begin{itemize}
%  \item It allows for the process to be \textcolor{red}{inverted}
%  \begin{itemize}
%    \item Prescribe the threshold $t$ \textcolor{dgreen}{in advance} and find the space spanned by all
%vectors $x$, such that $\vectornorm{Ax} \geq t$.
%    \item In this setting, computing the SVD is \textcolor{red}{not necessary} anymore!
%   \end{itemize}
%\end{itemize}

%\framebreak
Represent the \textcolor{dgreen}{frequency} of a direction (unit vector):
\begin{itemize}
  \item Assume the directions of $A$ are \textcolor{red}{indicator vectors} of the items: \\
  \begin{center}
  \vspace{2 mm}
  $A = \begin{pmatrix}
       1 & 0 & 0 & 0\\[0.3em] 
       0 & 1 & 0 & 0\\[0.3em]
       0 & 0 & 1 & 0\\[0.3em]
       0 & 1 & 0 & 0
     \end{pmatrix}$
  \end{center}
  \item Frequency of second item $e_2 = (0,1,0,0)^T$:
  $ \vectornorm{Ae_2}^2 = \vectornorm{(0,1,0,1)^T} ^2= 0^2 + 1^2 + 0^2 + 1^2 = 2$. 
  \item Generalize the directions to unit vector $\{x : \vectornorm{x}=1\} $ and the \textcolor{dgreen}{frequency} of a direction is $\vectornorm{Ax}^2$.
\end{itemize}

\framebreak
Connection to SVD of $A$:
\begin{itemize}
  \item $A = U\Sigma V^T \Leftrightarrow U^TA = \Sigma V^T \Leftrightarrow Au = \sigma v$.
  \item $\vectornorm{Au}^2 = \vectornorm{\sigma v}^2 = \sigma^2$.
\end{itemize}
  \vspace{2 mm}
Change $u$ to $x$: \\
  \vspace{2 mm}

The \textcolor{dgreen}{frequency} of a direction is indicated by the square of corresponding singular value $\sigma^2$.

\framebreak
The algorithm:
  \begin{algorithmic}
    \State \textbf{Input}: $\ell$, $A \in \mathbb{R}^{n\times m}$
    \State $B \leftarrow $ all zeros matrix $\in \mathbb{R}^{\ell\times m}$
    \For{$i =1,\ldots,n$} 
    \State Insert $i^{th}$ row of $A$ into zero valued row of $B$
    \If {$B$ has no zero valued rows}
      \State $[U,\Sigma,V] \leftarrow SVD(B)$
      \textcolor{gray}{\State $C \leftarrow \Sigma V^T$ // for proof}
      \State $\delta \leftarrow \sigma_{\ell/2}^2$
      \State $\breve{\Sigma} \leftarrow \sqrt{\max(\Sigma^2-I_\ell\delta,0)}$
      \State $B\leftarrow \breve{\Sigma}V^T$
    \EndIf
    \EndFor
  \end{algorithmic}
\end{frame}


\begin{frame}[allowframebreaks=1]{Frequent direction example}

\begin{align}
  A = \begin{pmatrix}
       0.9387 & 0.3447 \\[0.3em] 
       0.7605 & 0.6493 \\[0.3em]
       0.5858 & 0.8104 \\[0.3em]
       0.3413 & 0.9400
     \end{pmatrix} 
  \rightarrow
  B = \begin{pmatrix}
      0  & 0  \\[0.3em] 
       0 & 0
     \end{pmatrix} \nonumber
\end{align}

\framebreak

\begin{align}
  A = \begin{pmatrix}
       0.9387 & 0.3447 \\[0.3em] 
       0.7605 & 0.6493 \\[0.3em]
       0.5858 & 0.8104 \\[0.3em]
       0.3413 & 0.9400
     \end{pmatrix} 
  \rightarrow
  B = \begin{pmatrix}
       0.9387 & 0.3447 \\[0.3em] 
       0 & 0
     \end{pmatrix} 
\end{align}

\framebreak

\begin{align}
  A = \begin{pmatrix}
       0.9387 & 0.3447 \\[0.3em] 
       0.7605 & 0.6493 \\[0.3em]
       0.5858 & 0.8104 \\[0.3em]
       0.3413 & 0.9400
     \end{pmatrix} 
  \rightarrow
  B = \begin{pmatrix}
       0.9387 & 0.3447 \\[0.3em] 
       0.7605 & 0.6493
     \end{pmatrix} 
\end{align}

\framebreak
 $[U,\Sigma,V] \leftarrow SVD(B)$
\begin{align}
  B = \begin{pmatrix}
       0.9387 & 0.3447 \\[0.3em] 
       0.7605 & 0.6493
     \end{pmatrix} = \hspace{7cm} \nonumber  \\
\nonumber  \\
     \begin{pmatrix}
        \bovermat{U}{-0.7071 & -0.7071} \\[0.3em] 
       -0.7071 & 0.7071
     \end{pmatrix} 
    \begin{pmatrix}
       \bovermat{$\Sigma$}{1.3920 & 0.0000}\\[0.3em] 
       0.0000 & 0.2495
     \end{pmatrix} 
    \begin{pmatrix}
       \bovermat{$V^T$}{-0.8632 & -0.5049} \\[0.3em] 
       -0.5049 & 0.8632
     \end{pmatrix}   \nonumber
\end{align}

$\delta \leftarrow \sigma_{\ell/2}^2$
\begin{align}
   \ell/2 = 1, \delta = 1.3920^2 \nonumber
\end{align}

\framebreak
$\breve{\Sigma} \leftarrow \sqrt{\max(\Sigma^2-I_\ell\delta,0)}$
\begin{align}
 \breve\Sigma=\sqrt{\max(
      \begin{pmatrix}
       \bovermat{$\Sigma^2$}{1.3920^2 & 0.0000}\\[0.3em] 
       0.0000 & 0.2495^2
     \end{pmatrix} - 
      \begin{pmatrix}
       \bovermat{$I\delta$}{1.3920^2 & 0.0000}\\[0.3em] 
       0.0000 & 1.3920^2
     \end{pmatrix},0)} 
   \nonumber
\end{align}
$B\leftarrow \breve{\Sigma}V^T$
\begin{align}
  B = 
    \begin{pmatrix}
       \bovermat{$\breve\Sigma$}{0.0000 & 0.0000}\\[0.3em] 
       0.0000 & 0.0000
     \end{pmatrix}   
    \begin{pmatrix}
       \bovermat{$V$}{-0.8632 & -0.5049} \\[0.3em] 
       -0.5049 & 0.8632
     \end{pmatrix} ^T 
 = \begin{pmatrix}
       0 & 0 \\[0.3em] 
       0 & 0
      \end{pmatrix}  \nonumber 
\end{align}

\framebreak
\begin{align}
  A = \begin{pmatrix}
       0.9387 & 0.3447 \\[0.3em] 
       0.7605 & 0.6493 \\[0.3em]
       0.5858 & 0.8104 \\[0.3em]
       0.3413 & 0.9400
     \end{pmatrix} 
  \rightarrow
  B = \begin{pmatrix}
       0.5858 & 0.8104 \\[0.3em]
       0 & 0
     \end{pmatrix} 
\end{align}


\framebreak
\begin{align}
  A = \begin{pmatrix}
       0.9387 & 0.3447 \\[0.3em] 
       0.7605 & 0.6493 \\[0.3em]
       0.5858 & 0.8104 \\[0.3em]
       0.3413 & 0.9400
     \end{pmatrix} 
  \rightarrow
  B = \begin{pmatrix}
       0.5858 & 0.8104 \\[0.3em]
       0.3413 & 0.9400
     \end{pmatrix} 
\end{align}
\end{frame}


\begin{frame}[allowframebreaks=1]{Properties of the sketch matrix $B$}
In summary:
\begin{itemize}
  \item $A^TA \succeq B^TB \succeq 0$.
  \vspace{2 mm}
  \item $ ||A^TA - B^TB || \leq 2|| A||_f^2/\ell$. 
  \vspace{2 mm}
  \item Let $A = [A_1;A_2]$ and $B_1$, $B_2$ is the sketches of $A_1$ and $A_2$. A sketch $C$ of $B=[B_1;B_2]$ can be shown that:\\
  {\color{blue}\begin{align}
    ||A^TA - C^TC || \leq 2|| A||_f^2/\ell. \nonumber 
  \end{align}}
\end{itemize}
\end{frame}
%  \vspace{2 mm}
%1. $A^TA \succeq B^TB \succeq 0$. 
%\footnotesize
%\begin{proof}
%  {\color{blue}\begin{flalign}
%   & B^TB \succeq 0 \Leftrightarrow x^TB^TBx \geq 0 \Leftrightarrow \vectornorm{Bx}^2 \geq 0. && \nonumber \\
%   & A^TA \succeq B^TB  \Leftrightarrow x^T(A^TA-B^TB)x \geq 0 \Leftrightarrow \vectornorm{Ax}^2 -\vectornorm{Bx}^2 \geq 0. \nonumber &&
%  \end{flalign}}
%  {\color{blue}\begin{flalign}
%    \vectornorm{Ax}^2 -\vectornorm{Bx}^2 &= \sum_{i=1}^n[\langle A_i, x \rangle^2 + ||B^{i-1}x||^2 - ||B^{i}x||^2] \nonumber &&\\
%    &= \sum_{i=1}^n[||C^ix||^2 - ||B^{i}x||^2]= \sum_{i=1}^n[x^T(C^{i^T}C^i-B^{i^T}B^i)x] \nonumber &&\\
%    &= \sum_{i=1}^n[x^T(V\Sigma^T\Sigma V^T-V\breve{\Sigma}^T\breve{\Sigma} V^T)x] 
%    = \sum_{i=1}^n[x^TV(\Sigma^2 -\breve{\Sigma}^2)V^Tx] \geq 0. && \nonumber
%  \end{flalign}}
%\end{proof}


%\framebreak

%\normalsize
%  \vspace{2 mm}
%2. $ ||A^TA - B^TB || \leq 2|| A||_f^2/\ell$. 
%\footnotesize
%\begin{proof}
%  {\color{blue}\begin{flalign}
%    &||A^TA - B^TB || \leq \sum_{i=1}^n\delta_i \leq 2||A||_f^2/\ell. && \nonumber  
%  \end{flalign}}
%  {\color{blue}\begin{flalign}
%    ||A^TA - B^TB || &= \sigma_{max}(A^TA - B^TB) = x^T(A^TA-B^TB)x = \vectornorm{Ax}^2 -\vectornorm{Bx}^2  \nonumber && \\
%    &= \sum_{i=1}^n[||C^ix||^2 - ||B^{i}x||^2] \leq \sum_{i=1}^n[||C^{i^T}C^i - B^{i^T}B^i||] && \nonumber \\
%    &= \sum_{i=1}^n[||V(\Sigma^2 -\breve{\Sigma}^2) V^T||] =  \sum_{i=1}^n \delta_i. %\nonumber && 
%  \end{flalign}}
%\end{proof}

%\framebreak

%\normalsize
%  \vspace{2 mm}
%2. $ ||A^TA - B^TB || \leq 2|| A||_f^2/\ell$. 
%\footnotesize
%\begin{proof}
%  \begin{flalign}
%    &||A^TA - B^TB || \leq \sum_{i=1}^n\delta_i \leq 2||A||_f^2/\ell. && \nonumber 
%  \end{flalign}
%  {\color{blue}\begin{flalign}
%    ||B^n||_f^2 &= \sum_{i=1}^n[||B^{i-1}x||_f^2 - ||B^{i}||_f^2] = \sum_{i=1}^n[(||C^i||_f^2 - ||B^{i-1}x||_f^2) - (||C^i||_f^2 - ||B^{i}||_f^2)]. && \nonumber \\
%     &= \sum_{i=1}^n ||A_i||_f^2 - tr(C^{i^T}C^i - B^{i^T}B^i) = ||A||_f^2 - \sum_{i=1}^ntr(V(\Sigma^{i^2} -\breve{\Sigma}^{i^2}) V^T) && \nonumber \\
%     &= ||A||_f^2 - \sum_{i=1}^ntr(\Sigma^{i^2} -\breve{\Sigma}^{i^2}) \leq ||A||_f^2 - (\ell/2)\sum_{i=1}^n\delta_i \nonumber.
%  \end{flalign}}
%  {\color{blue}\begin{flalign}
%    \sum_{i=1}^n\delta_i \leq 2(||A||_f^2 - ||B||_f^2)/\ell \leq 2||A||_f^2/\ell. && \nonumber
%  \end{flalign}}
%\end{proof}
%\normalsize


%\framebreak


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Experiment section
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Content}
\begin{itemize}
\item \textcolor{gray}{Background}
\item \textcolor{gray}{Related work}
\item \textcolor{gray}{Frequent directions}
\item Experiments and Results
\item \textcolor{gray}{Conclusion}
\end{itemize}
\end{frame}


\begin{frame}[allowframebreaks=1]{Experiments}
Synthetic data $A \in \mathbb{R}^{n\times m}$
\begin{itemize}
  \item $A = SDU + N/\zeta$.
  \item $S \in \mathbb{R}^{n\times d},\quad S_{i,j} \sim \mathcal{N}(0,1)$.
  \item $D \in \mathbb{R}^{d\times d},\quad D_{i,i} = 1 - (i-1)/d$.
  \item $U \in \mathbb{R}^{d\times m}, \quad UU^T=I_d$.
  \item $c_1 \leq \zeta \leq c_2, \quad c_1 \approx 1,c_2 \approx 1$.
\end{itemize}
$A$ contains $d$ dimensional signal and $m$ dimensional noise, and is recoverable by spectral methods \cite{Vershynin11}.


\framebreak
Error against sketch size with $n=10000, m=1000, d=50$.
\begin{figure}
  \includegraphics[scale=0.6]{plots/acc}
 \label{fig:fp}
\end{figure}

\framebreak
Running time. \\
Left figure shows comparison with other techniques and right figure shows the linear behavior in $n$ and $m$.
\begin{figure}
  \subfigure{\includegraphics[scale=0.35]{plots/time1}}
  \subfigure{\includegraphics[scale=0.35]{plots/time2}}
 \label{fig:fgtree}
\end{figure}
\end{frame}

\begin{frame}[allowframebreaks=1]{More experiments}
\begin{figure}
  \includegraphics[scale=0.6]{plots/data_and_sketch}
 \label{fig:fp}
\end{figure}

\framebreak
Why that:\\
{\color{blue}\begin{flalign}
  A^TA = \begin{pmatrix}
       1.2485 & 0.0026 \\[0.3em] 
       0.0026 & 1.2638
     \end{pmatrix} \nonumber
\end{flalign}}
  Let $\hat{B} = B(1:2,1:2)$ \\
{\color{blue}\begin{flalign}
  \hat{B}^T\hat{B} = \begin{pmatrix}
       1.2214 & 0.0051 \\[0.3em] 
       0.0051 & 1.2369
     \end{pmatrix} \nonumber
\end{flalign}}
\end{frame}

\begin{frame}{What has been omitted from the presentation}

\begin{itemize}
	\item using \textcolor{red}{\textit{Frequent-directions}} to produce \textcolor{dgreen}{low rank approximations}
\end{itemize}

\end{frame}

%\begin{frame}{What has been omitted from the presentation}
%
%\begin{itemize}
	%\item using \textcolor{red}{\textit{Frequent-directions}} to produce \textcolor{dgreen}{low rank approximations}
%\end{itemize}
	%\item proof for the \textit{red}{sketching of sketches} property
%
%\end{frame}

\begin{frame}{Conclusion}
\begin{itemize}
  \item In terms of $|| A^TA - B^TB||$, the proposed sketching algorithm is \textcolor{red}{more accurate} than sampling, hashing and random projections.
  \item The proposed algorithm runs reasonably fast, \textcolor{red}{faster} than random projection, slower than sampling.
  \item The proposed algorithm is \textcolor{red}{linear} in the scale of the input size.
\end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bibliographystyle{plain}
\begin{frame}{References}
\small
\bibliography{presentation}
\end{frame}

\end{document}
