\documentclass[first=dgreen,second=purple,logo=redque]{aaltoslides}
%\documentclass{aaltoslides} % DEFAULT
%\documentclass[first=purple,second=lgreen,logo=redque,normaltitle,nofoot]{aaltoslides} % SOME OPTION EXAMPLES

\usepackage[latin9]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{amssymb,amsmath}
\usepackage{url}
\usepackage{lastpage}
\usepackage{subfigure}
\usepackage{algpseudocode}
%\usepackage{xcolor}

\definecolor{gold}{rgb}{0.3, 0.4, .1}
\definecolor{fore}{RGB}{249,242,215}
\definecolor{lgray}{RGB}{25, 25, 25}
\definecolor{back}{RGB}{51,51,51}
\definecolor{title}{RGB}{255,0,90}
\definecolor{dgreen}{rgb}{0.,0.6,0.}
\definecolor{gold}{rgb}{1.,0.84,0.}
\definecolor{JungleGreen}{cmyk}{0.99,0,0.52,0}
\definecolor{gold}{cmyk}{0.85,0,0.33,0}
\definecolor{RawSienna}{cmyk}{0,0.72,1,0.45}
\definecolor{Magenta}{cmyk}{0,1,0,0}

\title{Simple and Deterministic Matrix Sketching}

\author[H. Georgiev and H. Shen]{Hristo Georgiev and Huibin Shen}
\institute[ICS]{Department of Information and Computer Science\\
Aalto University, School of Science and Technology}

\aaltofootertext{H. Georgiev and H. Shen}{T-61.6020}{\arabic{page}/\pageref{LastPage}\ }

%\date{}
\newcommand{\vectornorm}[1]{\left\|#1\right\|}
\newcommand{\IndState}{\State\hspace{\algorithmicindent}}

\everymath\expandafter{\the\everymath \color{blue}}

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\aaltotitleframe

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Background section
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Content}
\begin{itemize}
\item Background
\item \textcolor{gray}{Related work}
\item \textcolor{gray}{Frequent directions}
\item \textcolor{gray}{Experiments and Results}
\item \textcolor{gray}{Conclusion}
\end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%\begin{frame}{Background}
%\begin{itemize}
%  \item Many applications are based on matrix, \emph{e.g.} search engine (document-term matrix), social networks (adjacency matrix).
%  \item In the ``Big data'' setting, matrix could be very large.
%  \item One way to handle massive data is to utilize map-reduce like programming model.
%\end{itemize}
%\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\begin{frame}{Background}
%\begin{itemize}
%  \item This paper: approximate the original matrix by a much smaller one while still preserving the correlation (if assume centering in the feature space). 
%  \item Formally, consider a large matrix $A \in \mathbb{R}^{n\times m}$ with $n$ rows and $m$ columns, a sketch matrix $B \in \mathbb{R}^{\ell \times m}$ containing only $\ell \ll n$ rows such that $A^TA \approx B^TB$. 

%This paper shows (Proof shown later):
%\begin{align}
%B^TB \prec A^TA \quad \text{and} \quad || A^TA-B^TB || \leq 2||A||_f^2/\ell \nonumber
%\end{align}
%\end{itemize}
%\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{What is a sketch?}
\begin{itemize}
  \item A sketch of a matrix $A$ is another matrix $B$ which is significantly
  smaller than $A$, but still approximates it well.
  \item A \textit{good} sketch matrix is one on which some computations can
  be performed, \textit{without} much loss of precision.
   \emph{(as opposed to performing them on the original matrix)}
\end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[allowframebreaks=1]{What would one need a sketch?}
\begin{itemize}
  \item Modern large data sets are often viewed as matrices
   \emph{(which are often extremely large)}
  \item{Examples:}
  \begin{itemize}
    \item Textual data in the bag-of-words model \emph{(Where the rows
    correspond to documents)}
    \item Large-scale image analysis \emph{(Each row corresponds to one image,
    and contains either pixel values, or other derived feature values)}
  \end{itemize}
  \item Low rank approximations are used in PCA, LSI, $k$-means clustering
\end{itemize}

\framebreak

\begin{itemize}
  \item Typically, compute the SVD of some large matrix $A$
  \item Then approximate using the first $k$ singular vectors
  \begin{itemize}
     \item where $k \geq t$, i.e. we are interested only in unit vectors such
     that $\vectornorm{Ax} \geq t$.
  \end{itemize}
  \item However, the distributed nature of such matrices renders SVD infeasible
\end{itemize}

\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Properties of matrix sketching methods}
Designed to be \textit{pass-efficient}
\begin{itemize}
\item i.e. data can be read only a \textit{constant} number of times.
\end{itemize}
The \textit{streaming model}: only one pass is permitted!
\begin{itemize}
\item Sketching becomes more challenging, since each row can be processed only
once. \emph{(further, storage is severely limited)}
\end{itemize}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Related work section
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Content}
\begin{itemize}
\item \textcolor{gray}{Background}
\item Related work
\item \textcolor{gray}{Frequent directions}
\item \textcolor{gray}{Experiments and Results}
\item \textcolor{gray}{Conclusion}
\end{itemize}
\end{frame}


\begin{frame}[allowframebreaks=1]{Existing approaches}
There are three main approaches:
\begin{itemize}
\item \textit{random-projection}
\item \textit{hashing}
\item \textit{sampling}
\end{itemize}

Proposed fourth approach, \textit{Frequent-directions}.

\framebreak

Random-projection: encaptures two classes of methods
\begin{enumerate}[(i)]
  \item Generate a sparser version of the matrix.
   \begin{itemize}
   \item It can then be stored \textit{more efficiently}, and
   \item can be multiplied \textit{faster} by other matrices.
    \end{itemize}
  \item Randomly combine matrix rows.
\end{enumerate}

\framebreak

Hashing:
\begin{itemize}
  \item Simple and efficient \textit{subspace embeddings}
     that can be applied in $O(nnz(A))$ time, for any matrix $A$.
\end{itemize}

\framebreak

Sampling in the context of the \textit{Column Subset Selection Problem}:
\begin{itemize}
  \item find a small subset of matrix rows (or columns) that approximates
  the entire matrix
  \item solved using a simple streaming solution:
   \begin{itemize}
   \item sample rows from the input matrix with probability proportional to
   their squared $\ell_{2}$ norm
   \end{itemize}
\end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Frequent directions section
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{frame}{Content}
\begin{itemize}
\item \textcolor{gray}{Background}
\item \textcolor{gray}{Related work}
\item Frequent directions
\item \textcolor{gray}{Experiments and Results}
\item \textcolor{gray}{Conclusion}
\end{itemize}
\end{frame}

\begin{frame}[allowframebreaks=1]{Item frequency estimation}
\begin{itemize}
    \item Used to uncover \textcolor{gold}{frequent} items in an item stream 
	\item Referred to as \textcolor{gold}{\textit{Frequent-items}}
	\item \textbf{Goal:} use $O(\ell)$ space to produce estimates $g_{j}$, such
	that
	\begin{itemize}\item{$|f_{j} - g_{j} | \leq n/l$}, for all $j$
	\textcolor{red}{simultaneously}\end{itemize}

\item The algorithm:
  \begin{algorithmic}
    \State \textbf{Input}: \textcolor{dgreen}{items} $a_{1}$, $a_{2}$, \ldots,
    $a_{m}$
    \IndState \textcolor{dgreen}{item appearances} $A_{1}$, $A_{2}$, \ldots,
    $A_{n}$
    \\\State \textbf{Repeat} until there are \textcolor{red}{less than}
    $\ell$ \textcolor{gold}{unique} items left \{
     \IndState Simulate a process of
     \textcolor{red}{\textit{'deleting'}} of $\ell$ appearances of
     \IndState \textcolor{gold}{different} items from the stream
    \State \}
  \end{algorithmic}
\end{itemize}

%   \begin{itemize}
%      \item The \textcolor{red}{trimmed} stream can be stored
%      \textcolor{gold}{concisely} in $O(\ell)$ space
%   \end{itemize}

\framebreak

\textbf{Claim:}

\begin{itemize}
\item \textbf{If} item $a_{j}$ appears in the \textcolor{gold}{final} trimmed
stream $g_{j}$ times
\begin{itemize}
  \item \textbf{Then} $g_{j}$ is a \textcolor{red}{good approximation} for its
  true frequency $f_{j}$ (even in the case of $g_{j}$ = 0).
\end{itemize}         
\end{itemize}

\textbf{Proof:}
\begin{itemize}
   \item Each item-type is deleted \textcolor{gold}{at most} once per iteration:
   \begin{itemize} \item $f_{j} - g_{j} \leq t$, where $t$ is the
   number of deleted items.\end{itemize}
\end{itemize}

\framebreak

\textbf{Proof (cont.):}

\begin{itemize}
   \item \textcolor{gray}{Each item-type is deleted gold at most
   once per iteration:}
   \begin{itemize} \item \textcolor{gray}{$f_{j} - g_{j} \leq t$, where $t$ is
   the number of deleted items.}\end{itemize}
   \item $\ell$ items are deleted in every batch, and the total number of
   deleted items is upper-bounded by $n$, therefore:
   \begin{itemize} \item $t\ell \leq n$, or $t \leq n/\ell$.\end{itemize} 
\end{itemize}

\begin{itemize}
  \item If one sets $\ell > 1/\varepsilon$,
  \begin{itemize} \item Then any item that appears more than
  $\varepsilon n$ times in the stream \textcolor{gold}{must} appear in
  the final sketch.\end{itemize}
\end{itemize}
  
\end{frame}

\begin{frame}[allowframebreaks=1]{\textit{Frequent-items} in matrix sketching}

\begin{itemize}
  \item \textbf{Input:} matrix $A$, consisting of a stream of
  \textcolor{gold}{rows} $A_{i}$,
  	\begin{itemize}\item where $A_{i} \in \{ e_{1}, \ldots, e_{m} \}$, for
  	$e_{j}$ being the $j$th \textcolor{gold}{standard basis} vector
  	
  	\item $A_{j} = e_{j}$, if the $i$th row of the matrix
  	\item Frequency: $f_{j} = \vectornorm{Ae_{j}}^{2}$\\
  \end{itemize}
  \item \textbf{Output:} $g_{j} = \vectornorm{Be_{j}}^{2}$, which is a
  	\textcolor{red}{good approximation} to $f_{j}$
  \begin{itemize}\item Let $n = \vectornorm{A}_{f}^{2}$, then $|f_{j} -
  g_{j}| \leq n / \ell$, which is equivalent to
  \begin{itemize}\item $\vectornorm{Ae_{j}}^{2} -
  \vectornorm{Be_{j}}^{2} \leq \vectornorm{A}_{j}^{2} / \ell$\end{itemize}
  \end{itemize}
\end{itemize}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[allowframebreaks=1]{Connection to \textit{sampling}}

\begin{itemize}
   \item \textcolor{red}{All} frequencies can be approximated from a
   \textcolor{gold}{uniform} sample of the stream
   \item using \textit{Chernoff's bound}, and then applying the \textit{union
   bound}
   \item $O(r\log{(r)}/\varepsilon^{2})$ samples suffice to ensure that
   \begin{itemize}
     \item $|f_{i} - g_{i}| \leq \varepsilon f_{\text{max}}$,
     \item where $r = n / f_{\text{max}}$
   \end{itemize}
\end{itemize}

\framebreak

\begin{itemize}
   \item \textcolor{red}{Sketching} by row sampling requires
   $O(r\log{(r)}/\varepsilon^{2})$ row samples,
   \begin{itemize}
     \item where $r = \vectornorm{A}_{f}^{2} /
   \vectornorm{A}_{2}^{2}$.
   	 \item This guarantees that $\vectornorm{A^{T}A - B^{T}B} \leq \varepsilon
   \vectornorm{A^{T}A}$.
   \end{itemize}
   \item \textbf{Note:} The matrix sampling result \textcolor{gold}{implies} the
   item sampling algorithm
   \item \emph{Because running the matrix sampling algorithm on an item
   indicator matrix (as before) produces uniform random samples.}
   \begin{itemize}
   \item We have $r = \vectornorm{A}_{f}^{2} / \vectornorm{A}_{2}^{2} =
   f_{\text{max}} / n$, and
   \item $f_{\text{max}} = \vectornorm{A^{T}A}$.\end{itemize}
   \item \textit{Frequent-directions} \textcolor{red}{improves} on matrix
   sampling
   \begin{itemize} \item in the \textcolor{gold}{same way} that
   \textit{Frequent-items} improves on item sampling.\end{itemize}
\end{itemize}

\end{frame}

\begin{frame}[allowframebreaks=1]{Connection to \textit{low-rank approximation}}

\begin{itemize}
   \item The goal is to obtain a \textcolor{red}{small} matrix $B$,
   \begin{itemize}
     \item consisting of $\ell$ rows, that contains in its row space
   	 \item a projection matrix $\Pi_{B,\xi}^{A}$ of rank $k$ such that 
   	 \begin{itemize}
   	   \item $\vectornorm{A - A\Pi_{B,\xi}^{A}}_{\xi} \leq (1+\varepsilon)
   	   \vectornorm{A - A_{k}}_{\xi}$, 

   	     	\item where $A_{k}$ is the \textcolor{red}{best}
   	   rank-$k$ approximation of $A$, and
   	   		\item $\xi$ is either $2$ or $f$\emph{(i.e. either the spectral norm or
   	   the Frobenius norm)}.
   	 \end{itemize}
   \end{itemize}
   \item \textbf{Note:} \textit{Frequent-directions} can be used to
   \textcolor{red}{produce} a low-rank approximation result.
   \begin{itemize} \item \textit{Lemma 4} from a modified version
   of [12].\end{itemize}
\end{itemize}

\framebreak

\textbf{Lemma: }
\begin{itemize}
    \item \ldots
\end{itemize}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[allowframebreaks=1]{The \textit{Frequent-directions} algorithm}
\begin{itemize}
  \item It allows for the process to be inverted
  \begin{itemize}
    \item Prescribe the threshold $t$ in advance and find the space spanned by all
vectors $x$, such that $\vectornorm{Ax} \geq t$.
    \item In this setting, computing the SVD is not necessary anymore!
   \end{itemize}
\end{itemize}

\framebreak
Represent the frequency of a row (direction):
\begin{itemize}
  \item Assume rows of $A$ are indicate vectors of the items: \\
  \begin{center}
  \vspace{2 mm}
  $A = \begin{pmatrix}
       1 & 0 & 0 & 0\\[0.3em] 
       0 & 1 & 0 & 0\\[0.3em]
       0 & 0 & 1 & 0\\[0.3em]
       0 & 1 & 0 & 0
     \end{pmatrix}$
  \end{center}
  \item Frequency of second item $e_2 = (0,1,0,0)^T$:
  $ \vectornorm{Ae_2}^2 = \vectornorm{(0,1,0,1)^T} ^2= 0^2 + 1^2 + 0^2 + 1^2 = 2$. 
  \item Generalize the rows (directions) to $\{x : \vectornorm{x}=1\} $ and the frequency of a direction is $\vectornorm{Ax}^2$.
\end{itemize}

\framebreak
Connection to SVD of $A$:
\begin{itemize}
  \item $A = U\Sigma V^T \Leftrightarrow U^TA = \Sigma V^T \Leftrightarrow Au = \sigma v$.
  \item $\vectornorm{Au}^2 = \vectornorm{\sigma v}^2 = \sigma^2$.
\end{itemize}
  \vspace{2 mm}
Change $u$ to $x$: \\
  \vspace{2 mm}

The frequency of a direction is indicated by the square of corresponding singular value $\sigma^2$.

\framebreak
The algorithm:
  \begin{algorithmic}
    \State \textbf{Input}: $\ell$, $A \in \mathbb{R}^{n\times m}$
    \State $B \leftarrow $ all zeros matrix $\in \mathbb{R}^{\ell\times m}$
    \For{$i =1,\ldots,n$} 
    \State Insert $i^{th}$ row of $A$ into zero valued row of $B$
    \If {$B$ has no zero valued rows}
      \State $[U,\Sigma,V] \leftarrow SVD(B)$
      \textcolor{gray}{\State $C \leftarrow \Sigma V^T$ // for proof}
      \State $\delta \leftarrow \sigma_{\ell/2}^2$
      \State $\breve{\Sigma} \leftarrow \sqrt{\max(\Sigma^2-I_\ell\delta,0)}$
      \State $B\leftarrow \breve{\Sigma}V^T$
    \EndIf
    \EndFor
  \end{algorithmic}
\end{frame}

\begin{frame}[allowframebreaks=1]{Properties of the sketch matrix $B$}
  \vspace{2 mm}
1. $A^TA \succeq B^TB \succeq 0$. 
\footnotesize
\begin{proof}
  {\color{blue}\begin{flalign}
   & B^TB \succeq 0 \Leftrightarrow x^TB^TBx \geq 0 \Leftrightarrow \vectornorm{Bx}^2 \geq 0. && \nonumber \\
   & A^TA \succeq B^TB  \Leftrightarrow x^T(A^TA-B^TB)x \geq 0 \Leftrightarrow \vectornorm{Ax}^2 -\vectornorm{Bx}^2 \geq 0. \nonumber &&
  \end{flalign}}
  {\color{blue}\begin{flalign}
    \vectornorm{Ax}^2 -\vectornorm{Bx}^2 &= \sum_{i=1}^n[\langle A_i, x \rangle^2 + ||B^{i-1}x||^2 - ||B^{i}x||^2] \nonumber &&\\
    &= \sum_{i=1}^n[||C^ix||^2 - ||B^{i}x||^2]= \sum_{i=1}^n[x^T(C^{i^T}C^i-B^{i^T}B^i)x] \nonumber &&\\
    &= \sum_{i=1}^n[x^T(V\Sigma^T\Sigma V^T-V\breve{\Sigma}^T\breve{\Sigma} V^T)x] 
    = \sum_{i=1}^n[x^TV(\Sigma^2 -\breve{\Sigma}^2)V^Tx] \geq 0. && \nonumber
  \end{flalign}}
\end{proof}


\framebreak

\normalsize
  \vspace{2 mm}
2. $ ||A^TA - B^TB || \leq 2|| A||_f^2/\ell$. 
\footnotesize
\begin{proof}
  {\color{blue}\begin{flalign}
    &||A^TA - B^TB || \leq \sum_{i=1}^n\delta_i \leq 2||A||_f^2/\ell. && \nonumber  
  \end{flalign}}
  {\color{blue}\begin{flalign}
    ||A^TA - B^TB || &= \sigma_{max}(A^TA - B^TB) = x^T(A^TA-B^TB)x = \vectornorm{Ax}^2 -\vectornorm{Bx}^2  \nonumber && \\
    &= \sum_{i=1}^n[||C^ix||^2 - ||B^{i}x||^2] \leq \sum_{i=1}^n[||C^{i^T}C^i - B^{i^T}B^i||] && \nonumber \\
    &= \sum_{i=1}^n[||V(\Sigma^2 -\breve{\Sigma}^2) V^T||] =  \sum_{i=1}^n \delta_i. \nonumber && 
  \end{flalign}}
\end{proof}

\framebreak

\normalsize
  \vspace{2 mm}
2. $ ||A^TA - B^TB || \leq 2|| A||_f^2/\ell$. 
\footnotesize
\begin{proof}
%  \begin{flalign}
%    &||A^TA - B^TB || \leq \sum_{i=1}^n\delta_i \leq 2||A||_f^2/\ell. && \nonumber 
%  \end{flalign}
  {\color{blue}\begin{flalign}
    ||B^n||_f^2 &= \sum_{i=1}^n[||B^{i-1}x||_f^2 - ||B^{i}||_f^2] = \sum_{i=1}^n[(||C^i||_f^2 - ||B^{i-1}x||_f^2) - (||C^i||_f^2 - ||B^{i}||_f^2)]. && \nonumber \\
     &= \sum_{i=1}^n ||A_i||_f^2 - tr(C^{i^T}C^i - B^{i^T}B^i) = ||A||_f^2 - \sum_{i=1}^ntr(V(\Sigma^{i^2} -\breve{\Sigma}^{i^2}) V^T) && \nonumber \\
     &= ||A||_f^2 - \sum_{i=1}^ntr(\Sigma^{i^2} -\breve{\Sigma}^{i^2}) \leq ||A||_f^2 - (\ell/2)\sum_{i=1}^n\delta_i \nonumber.
  \end{flalign}}
  {\color{blue}\begin{flalign}
    \sum_{i=1}^n\delta_i \leq 2(||A||_f^2 - ||B||_f^2)/\ell \leq 2||A||_f^2/\ell. && \nonumber
  \end{flalign}}
\end{proof}
\normalsize


\framebreak
In summary:
\begin{itemize}
  \item $A^TA \succeq B^TB \succeq 0$.
  \vspace{2 mm}
  \item $ ||A^TA - B^TB || \leq 2|| A||_f^2/\ell$. 
  \vspace{2 mm}
  \item Let $A = [A_1;A_2]$ and $B_1$, $B_2$ is the sketches of $A_1$ and $A_2$. A sketch $C$ of $B=[B_1;B_2]$ can be shown that (proof omitted):\\
  {\color{blue}\begin{align}
    ||A^TA - C^TC || \leq 2|| A||_f^2/\ell. \nonumber 
  \end{align}}
\end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Experiment section
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Content}
\begin{itemize}
\item \textcolor{gray}{Background}
\item \textcolor{gray}{Related work}
\item \textcolor{gray}{Frequent directions}
\item Experiments and Results
\item \textcolor{gray}{Conclusion}
\end{itemize}
\end{frame}


\begin{frame}[allowframebreaks=1]{Experiments}
Synthetic data $A \in \mathbb{R}^{n\times m}$
\begin{itemize}
  \item $A = SDU + N/\zeta$.
  \item $S \in \mathbb{R}^{n\times d},\quad S_{i,j} \sim \mathcal{N}(0,1)$.
  \item $D \in \mathbb{R}^{d\times d},\quad D_{i,i} = 1 - (i-1)/d$.
  \item $U \in \mathbb{R}^{d\times m}, \quad UU^T=I_d$.
  \item $c_1 \leq \zeta \leq c_2, \quad c_1 \approx 1,c_2 \approx 1$.
\end{itemize}
$A$ contains $d$ dimensional signal and $m$ dimensional noise, and is recoverable by spectral methods \cite{Vershynin11}.


\framebreak
Error against sketch size with $n=10000, m=1000, d=50$.
\begin{figure}
  \includegraphics[scale=0.6]{plots/acc}
 \label{fig:fp}
\end{figure}

\framebreak
Running time. \\
Left figure shows comparison with other techniques and right figure shows the linear behavior in $n$ and $m$.
\begin{figure}
  \subfigure{\includegraphics[scale=0.35]{plots/time1}}
  \subfigure{\includegraphics[scale=0.35]{plots/time2}}
 \label{fig:fgtree}
\end{figure}
\end{frame}

\begin{frame}[allowframebreaks=1]{More experiments}
\begin{figure}
  \includegraphics[scale=0.6]{plots/data_and_sketch}
 \label{fig:fp}
\end{figure}

\framebreak
Why that:\\
{\color{blue}\begin{flalign}
  A^TA = \begin{pmatrix}
       1.2485 & 0.0026 \\[0.3em] 
       0.0026 & 1.2638
     \end{pmatrix} \nonumber
\end{flalign}}
  Let $\hat{B} = B(1:2,1:2)$ \\
{\color{blue}\begin{flalign}
  \hat{B}^T\hat{B} = \begin{pmatrix}
       1.2214 & 0.0051 \\[0.3em] 
       0.0051 & 1.2369
     \end{pmatrix} \nonumber
\end{flalign}}
\end{frame}

\begin{frame}{Conclusion}
\begin{itemize}
  \item In terms of $|| A^TA - B^TB||$, the proposed sketching algorithm is more accurate than sampling, hashing and random projections.
  \item The proposed algorithm runs reasonably fast, faster than random projection, slower than sampling.
  \item The proposed algorithm is linear in the scale of the input size.
\end{itemize}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibliographystyle{plain}
\begin{frame}{Reference}
\small
\bibliography{presentation}
\end{frame}

\end{document}
